{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "In the epidemiological forecasting challenges that have run over the last 5 to 10 years, specific scores such as the log score and weighted interval score have been used for evaluating forecast skill. There is some mathematical theory to support the use of these scores. Specifically, they are proper scoring rules, meaning that if data are generated from a particular process, in expectation the forecasts corresponding to the data generating process will optimize the score. However, it is not clear how well these scores measure aspects of the forecasts that epidemiologists care about, or whether they might encourage or reward some undesired behavior in forecasts. The goal of this project is to develop better understanding of the implications of using these scoring rules to evaluate forecasts, and to develop or explore alternatives.\n",
    "\n",
    "# Decision Theoretic Set Up\n",
    "\n",
    "Currently, infectious disease forecasting exercises separate the evaluation of forecasts from the ways in which the forecasts may actually be put to use. Here, we more directly frame forecast scoring in a decision theoretic set up that captures the potential costs of decisions that are made using forecasts as inputs. Importantly, different forecast errors (such as predicting no growth when in fact exponential growth occurs, or predicting continued exponential growth when in fact trends in incidence reverse) may be more costly for some purposes than others. We begin by giving a general set up, and then hypothesize some possible cost functions that may be relevant to public health decision makers.\n",
    "\n",
    "To make the ideas more concrete, we introduce a running example of an agency planning the distribution of a limited set of supplies across multiple geographic units. For example, we may have a limited supply of personal protective equipment or oxygen that needs to be distributed across multiple hospitals in a state. For the purpose of this article, we assume that the agency's goal is to obtain a distribution of these limited resources that minimizes an expected loss; for example, we may wish to minimize the expected number of hospital patients in need of oxygen but without access to it. This is referred to as the *Bayes action*. We note that there are other possible strategies, such as minimizing the worst-case outcome for number of patients without access to oxygen, i.e., the *minimax action*.\n",
    "\n",
    "We index the locations under consideration by $l = 1, \\ldots, L$. The vector $Y_t$ records the outcome of interest at time $t$ across all locations; for instance, this may be the number of severe hospitalizations that occur at time $t$ in each hospital system. Let $\\mathcal{Y}$ denote the space of possible values for $Y_t$, and $y$ a particular element of $\\mathcal{Y}$. Similarly, let $\\mathcal{A}$ denote the space of possible actions, and $a$ an element of $\\mathcal{A}$. For example, $a$ may represent a particular allocation of PPE or oxygen to different hospitals represented by the supply of oxygen that is supplied to each hospital. If only $M$ resources are available, the action space is the set $\\mathcal{A} = \\{ (a_1, \\ldots, a_L): a_l \\geq 0, \\sum_{l=1}^L a_l = M \\}$. The loss incurred by taking action $a$ when outcome $y$ occurs is denoted by $L(y, a)$. Finally, let $\\mathcal{P}$ be a convex set of probability measures on $\\mathcal{Y}$, and $a_P \\in \\mathcal{A}$ be the *Bayes action* for a measure $P \\in \\mathcal{P}$:\n",
    "$$a_P = \\text{argmin}_{a \\in \\mathcal{A}} \\mathbb{E}_P[L(y, a)] = \\text{argmin}_{a \\in \\mathcal{A}} \\int L(y, a) \\, d P(y).$$\n",
    "\n",
    "We posit here a specific example of a loss function that may reasonably describe the setting of distributing a limited supply of resources to each of $L$ hospitals:\n",
    "$$\n",
    "L(y, a) = c \\sum_{l = 1}^L \\max(0, y_l - a_l)\n",
    "$$\n",
    "In other words, if $y_l > a_l$, we incur a loss proportional to the difference between the disease burden in location $l$ and the supply of resources that were directed to location $l$. On the other hand, there is no direct loss incurred if more resources than necessary are directed to location $l$, other than the implied reduction in resources available for other locations.\n",
    "\n",
    "Forecasts are often evaluated using proper scoring rules. A scoring rule is a function $S: \\mathcal{P} \\times \\mathcal{Y} \\rightarrow \\overline{\\mathbb{R}}$ that maps a probability measure $P$ and observed outcome $y$ to a score that intuitively measures how consistent $y$ is with the distribution $P$. A proper scoring rule is a scoring rule such that if data are generated from a distribution $Q$, the expected score is minimized by $Q$: $\\mathbb{E}_Q[S(Q, Y)] \\leq \\mathbb{E}_Q[S(P, Y)]$ for all $P \\in \\mathcal{P}$. Restating for concreteness, the condition is that $\\int S(Q, y) \\, d Q(y) \\leq \\int S(P, y) \\, d Q(y)$ for all $P \\in \\mathcal{P}$.\n",
    "\n",
    "Two commonly used scoring rules are the log score (LS) and the continuous ranked probability score (CRPS): \n",
    "$$\n",
    "\\begin{align*}\n",
    "S_{LS}(P, y) &= -\\log[p(y)] \\text{, where $p$ is the density of $P$} \\\\\n",
    "S_{CRPS}(P, y) &= \\int_{-\\infty}^{\\infty} [F_P(z) - \\mathbf{1}_{[y, \\infty)}(z)]^2 \\, d z \\text{, where $F_P$ is the cumulative distribution function of $P$}\n",
    "\\end{align*}\n",
    "$$\n",
    "Additionally, as noted by Gneiting and Raftery and apparently discussed in more detail by Dawid (haven't read it yet), in a decision theoretic setting with a loss function $L$, the score defined by $S_{L}(P, y) = L(y, a_P)$ is a proper score. **Question: is it possible to reinterpret the log score and CRPS as coming out of this decision theoretic set up with some implied or \"default\" action?**\n",
    "\n",
    "Given a set of $n$ observations, we can evaluate the forecast rule $P$ (alert: notational abuse) by calculating the mean score on the observed data: $\\frac{1}{n} \\sum_i S(P, y_i)$. As $n \\rightarrow \\infty$, this mean score approaches the expected score, which is minimized by the predictive distribution associated with the data generating process if the score is proper. As a result, asymptotically, the expected scores behave the same in expectation. However, there may still be other important differences between the scores: asymptotically, they may behave differently in terms of quantities other than the first moment; and even the first moment may behave differently with small sample sizes.\n",
    "\n",
    "# Understanding implications of different loss/utility functions for optimal forecasts\n",
    "\n",
    "Our basic goal is to understand the behavior of an optimal forecaster under different loss functions. Our understanding of the theory of proper scoring rules is that if a proper scoring rule is used and the data generating process is in the set of models under consideration, that process will ``beat\" all other models in expectation (i.e., as the sample size of our evaluation set goes to infinity). However, what happens if (a) the data generating process is not among the candidate models [note that in practice, the best performing models are typically much simpler than the data generating process]; and (b) our sample size is limited?\n",
    "\n",
    "Can we make any statements, either through formal derivation or simulation studies, about differences between the optimal forecasts under different [proper] scoring rules? We divide this question into two sub-parts:\n",
    "\n",
    "1. Are there any situations in which a commonly used proper scoring rule leads to predictions that we find intuitively unappealing?\n",
    "2. Do more carefully designed scoring rules that match targeted use cases result in qualitatively different optimal forecasts?\n",
    "\n",
    "## Possible downsides of common scoring rules\n",
    "\n",
    "Do common scoring rules reward behavior that is not desirable from the perspective of an epidemiologist who wants to use forecasts as an input to some decision? We have thought about two ways this might come about:\n",
    "1. Scoring rules that equally reward performance at all time points may encourage forecasters to either predict no growth, or maybe to predict a continuation of recent trends in growth. Changes in growth rate are relatively rare; most of the time, a forecaster can do well by predicting a continuation of recent trends. But maybe what an epidemiologist cares about most is when trends are going to change.\n",
    "2. Common scoring rules may encourage some kind of shrinkage that might encourage downward-biased forecasts of disease incidence in the largest locations or at the peaks in incidence.  Aaron Gerding found this [blog post](https://jmanton.wordpress.com/?s=james-stein) that briefly states the situation and concerns here.\n",
    "3. Possibly related to the previous point, commonly used scoring rules may encourage forecasts with an attenuated slope?\n",
    "\n",
    "Here are some expanded thoughts on the second point:\n",
    "\n",
    "### Connection between log score and MSE in a simple case\n",
    "\n",
    "Suppose $X_1, \\ldots, X_T \\sim \\text{Normal}_p(\\theta, \\sigma^2 \\mathbb{I})$ are i.i.d. $p$-dimensional random variables, with the mean vector $\\theta$ unknown and, for now, $\\sigma^2$ known. Our goal is to study estimators of the distribution of a new random variate $X_{T+1}$ drawn from this same distribution.  We will denote such a distributional estimator by $\\widehat{\\mathcal{D}}(X_1, \\ldots, X_T)$ to emphasize that the estimator is a function of the random variables $X_1, \\ldots, X_T$, or just $\\widehat{\\mathcal{D}}$ for brevity.  If we have an estimator $\\widehat{\\theta}$ of the mean vector $\\theta$, a corresponding distributional estimator is given by $\\widehat{\\mathcal{D}} = \\text{Normal}_p(\\widehat{\\theta}, \\sigma^2 \\mathbb{I})$.\n",
    "\n",
    "Given observed values $x_1, \\ldots, x_{T+1}$, we could measure the quality of the distributional estimate $\\widehat{d} = \\widehat{d}(x_1, \\ldots, x_t)$ by the negative log sore:\n",
    "$$NLS(\\widehat{d}, x_{T+1}) = -\\log\\left[ f(x_{T+1} \\vert \\widehat{\\theta}, \\sigma^2\\mathbb{I} ) \\right],$$\n",
    "where $f$ is the pdf of a multivariate normal distribution. In general, our goal is to select an estimator that minimizes the expected negative log score (NLS):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R}_{NLS}(\\widehat{\\mathcal{D}}) &= \\mathbb{E}_{X_{1:{T+1}}}\\left[ NLS(\\widehat{\\mathcal{D}}, X_{T+1}) \\right] \\\\\n",
    "&= \\int \\cdots \\int_{\\mathcal{X}} -\\log\\left[ f(x_{T+1} \\vert \\widehat{\\theta}(x_1, \\ldots, x_T), \\sigma^2\\mathbb{I} ) \\right] \\, d F_\\theta(x_1 \\cdots x_{T+1}),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where subscript $X_{1:{T+1}}$ indicates that the expectation is with respect to the distribution of $X_1, \\ldots, X_{T+1}$, and $F_\\theta$ is the distribution of these random variables.\n",
    "\n",
    "In the context of point estimation of the mean vector $\\theta$, we could also measure the quality of the estimator $\\widehat{\\theta}$ via the mean squared error:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{R}_{MSE}(\\widehat{\\theta}) &= \\mathbb{E}_{X_{1:T}}\\left[\\sum_{j = 1}^p (\\widehat{\\theta}_j - \\theta_j)^2 \\right] \\\\\n",
    "&= \\int \\cdots \\int{\\mathcal{X}} \\sum_{j = 1}^p \\left[\\widehat{\\theta}(x_1, \\ldots, x_T)_j - \\theta_j\\right]^2 \\, d F_\\theta(x_1 \\cdots x_{T}).\n",
    "\\end{align}\n",
    "$$\n",
    "Since this expression only evaluates the risk of the estimator of $\\theta$ based on the first $T$ observations, the expection is only with respect to the distribution of $X_1, \\ldots, X_T$.\n",
    "\n",
    "In fact, these two risk functions are essentially equivalent:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{R}_{NLS}(\\widehat{\\mathcal{D}}) &= \\mathbb{E}_{X_{1:{T+1}}}\\left[ -\\log\\left\\{ f(X_{T+1} \\vert \\widehat{\\theta}(X_1, \\ldots, X_T), \\sigma^2\\mathbb{I} ) \\right\\} \\right] \\\\\n",
    "&= \\mathbb{E}_{X_{1:{T+1}}}\\left[ - \\sum_{j = 1}^p \\left\\{ c - \\frac{1}{2 \\sigma^2} (X_{T+1, j} - \\widehat{\\theta}_j)^2 \\right\\} \\right] \\\\\n",
    "&= -pc + \\frac{1}{2 \\sigma^2} \\sum_{j = 1}^p \\mathbb{E}_{X_{1:{T+1}}}\\left[ (X_{T+1, j} - \\widehat{\\theta}_j)^2 \\right] \\\\\n",
    "&= -pc + \\frac{1}{2 \\sigma^2} \\sum_{j = 1}^p \\mathbb{E}_{X_{1:{T+1}}}\\left[ \\left\\{(X_{T+1, j} - \\theta_j) + (\\theta_j - \\widehat{\\theta}_j)\\right\\}^2 \\right] \\\\\n",
    "&= -pc + \\frac{1}{2 \\sigma^2} \\sum_{j = 1}^p \\left[ \\mathbb{E}_{X_{1:{T+1}}}\\left\\{(X_{T+1, j} - \\theta_j)^2\\right\\} + 2 \\mathbb{E}_{X_{1:{T+1}}}\\left\\{(X_{T+1, j} - \\theta_j)(\\theta_j - \\widehat{\\theta}_j)\\right\\} + \\mathbb{E}_{X_{1:{T+1}}}\\left\\{(\\theta_j - \\widehat{\\theta}_j)^2\\right\\} \\right] \\\\\n",
    "&= -pc + \\frac{1}{2 \\sigma^2} \\left[ p\\sigma^2 + \\mathcal{R}_{MSE}(\\widehat{\\theta}) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Connection to James-Stein and MSE Optimality\n",
    "\n",
    "In the simple setting above, the James-Stein estimator and similar shrinkage estimators are known to achieve smaller MSE than the maximum likelihood estimator.  Denote such a shrinkage estimator of $\\theta$ and the corresponding estimator of the distribution of $X_{T+1}$ by $\\widehat{\\theta}^{S}$ and $\\widehat{\\mathcal{D}}^{S}$ respectively, and the similar estimators obtained from maximum likelihood by $\\widehat{\\theta}^{MLE}$ and $\\widehat{\\mathcal{D}}^{MLE}$.  The James-Stein result is that $\\mathcal{R}_{MSE}(\\widehat{\\theta}^S) \\leq \\mathcal{R}_{MSE}(\\widehat{\\theta}^{MLE})$, with a strict inequality for almost all values of $\\theta$. Combining this result with the equivalence between negative log score and MSE in Equation~\\eqref{eqn:risk_equivalence} above, we see that also $\\mathcal{R}_{NLS}(\\widehat{\\mathcal{D}}^{S}) \\leq \\mathcal{R}_{NLS}(\\widehat{\\mathcal{D}}^{MLE})$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulation Study - One Time Point\n",
    "\n",
    "We consider a first very simple simulation study based on the conditions described above, with a single fixed time point and multiple locations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Z_t &\\sim \\text{Normal}_L(\\theta, \\sigma^2 \\mathbb{I}) \\\\\n",
    "Y_{lt} &= Z_{lt} \\cdot \\text{pop}_l\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We fix $L = 50$ and choose $\\theta$ to be distributed between 10 and 60; these are plausible values for hospitalizations per 100,000 population. We assume $\\sigma$ is known to be 1. The number of hospitalized people in location $l$, $Y_{lt}$, is calculated as the rate of hospitalizations per 100,000 population, $Z_{lt}$, times the population in location $l$ in units of 100,000 people.\n",
    "\n",
    "We measure distributional forecast skill through the three proper scores outlined above: $S_{LS}$, $S_{CRPS}$, and $S_L$ using the application-specific loss based on unmet hospitalization needs.\n",
    "\n",
    "We consider two estimators of $\\theta$, based on a single observation of length 50 from the target distribution:\n",
    "1. the maximum likelihood estimator, which sets $\\widehat{\\theta} = y$\n",
    "2. an empirical linear Bayes shrinkage estimator proposed by Efron and Morris"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import properscoring as ps\n",
    "from scipy.stats import norm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "location_pops = pd.read_csv(\"https://raw.githubusercontent.com/reichlab/covid19-forecast-hub/master/data-locations/locations.csv\")\n",
    "location_pops"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>location</th>\n",
       "      <th>location_name</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>US</td>\n",
       "      <td>332875137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>01</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4903185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>02</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>731545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>04</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7278717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>05</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3017804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56037</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>42343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56039</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>23464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56041</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>20226.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56043</td>\n",
       "      <td>Washakie County</td>\n",
       "      <td>7805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56045</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>6927.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3202 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abbreviation location      location_name   population\n",
       "0              US       US                 US  332875137.0\n",
       "1              AL       01            Alabama    4903185.0\n",
       "2              AK       02             Alaska     731545.0\n",
       "3              AZ       04            Arizona    7278717.0\n",
       "4              AR       05           Arkansas    3017804.0\n",
       "...           ...      ...                ...          ...\n",
       "3197          NaN    56037  Sweetwater County      42343.0\n",
       "3198          NaN    56039       Teton County      23464.0\n",
       "3199          NaN    56041       Uinta County      20226.0\n",
       "3200          NaN    56043    Washakie County       7805.0\n",
       "3201          NaN    56045      Weston County       6927.0\n",
       "\n",
       "[3202 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "state_pops = location_pops \\\n",
    "    .loc[\n",
    "        (location_pops.location <= '56') &\n",
    "        (location_pops.location.str.len() <= 2) &\n",
    "        (location_pops.abbreviation != 'DC')] \\\n",
    "    .assign(pop_100k = lambda x: x.population / 100000.)\n",
    "state_pops"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>location</th>\n",
       "      <th>location_name</th>\n",
       "      <th>population</th>\n",
       "      <th>pop_100k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>01</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4903185.0</td>\n",
       "      <td>49.03185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AK</td>\n",
       "      <td>02</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>731545.0</td>\n",
       "      <td>7.31545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>04</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>7278717.0</td>\n",
       "      <td>72.78717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>05</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3017804.0</td>\n",
       "      <td>30.17804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CA</td>\n",
       "      <td>06</td>\n",
       "      <td>California</td>\n",
       "      <td>39512223.0</td>\n",
       "      <td>395.12223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CO</td>\n",
       "      <td>08</td>\n",
       "      <td>Colorado</td>\n",
       "      <td>5758736.0</td>\n",
       "      <td>57.58736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CT</td>\n",
       "      <td>09</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>3565287.0</td>\n",
       "      <td>35.65287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DE</td>\n",
       "      <td>10</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>973764.0</td>\n",
       "      <td>9.73764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>FL</td>\n",
       "      <td>12</td>\n",
       "      <td>Florida</td>\n",
       "      <td>21477737.0</td>\n",
       "      <td>214.77737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GA</td>\n",
       "      <td>13</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>10617423.0</td>\n",
       "      <td>106.17423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HI</td>\n",
       "      <td>15</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1415872.0</td>\n",
       "      <td>14.15872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID</td>\n",
       "      <td>16</td>\n",
       "      <td>Idaho</td>\n",
       "      <td>1787065.0</td>\n",
       "      <td>17.87065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IL</td>\n",
       "      <td>17</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>12671821.0</td>\n",
       "      <td>126.71821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IN</td>\n",
       "      <td>18</td>\n",
       "      <td>Indiana</td>\n",
       "      <td>6732219.0</td>\n",
       "      <td>67.32219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IA</td>\n",
       "      <td>19</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>3155070.0</td>\n",
       "      <td>31.55070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KS</td>\n",
       "      <td>20</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>2913314.0</td>\n",
       "      <td>29.13314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>4467673.0</td>\n",
       "      <td>44.67673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LA</td>\n",
       "      <td>22</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>4648794.0</td>\n",
       "      <td>46.48794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ME</td>\n",
       "      <td>23</td>\n",
       "      <td>Maine</td>\n",
       "      <td>1344212.0</td>\n",
       "      <td>13.44212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>6045680.0</td>\n",
       "      <td>60.45680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MA</td>\n",
       "      <td>25</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>6892503.0</td>\n",
       "      <td>68.92503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MI</td>\n",
       "      <td>26</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>9986857.0</td>\n",
       "      <td>99.86857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MN</td>\n",
       "      <td>27</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>5639632.0</td>\n",
       "      <td>56.39632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MS</td>\n",
       "      <td>28</td>\n",
       "      <td>Mississippi</td>\n",
       "      <td>2976149.0</td>\n",
       "      <td>29.76149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MO</td>\n",
       "      <td>29</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>6626371.0</td>\n",
       "      <td>66.26371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>MT</td>\n",
       "      <td>30</td>\n",
       "      <td>Montana</td>\n",
       "      <td>1068778.0</td>\n",
       "      <td>10.68778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NE</td>\n",
       "      <td>31</td>\n",
       "      <td>Nebraska</td>\n",
       "      <td>1934408.0</td>\n",
       "      <td>19.34408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NV</td>\n",
       "      <td>32</td>\n",
       "      <td>Nevada</td>\n",
       "      <td>3080156.0</td>\n",
       "      <td>30.80156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NH</td>\n",
       "      <td>33</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>1359711.0</td>\n",
       "      <td>13.59711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NJ</td>\n",
       "      <td>34</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>8882190.0</td>\n",
       "      <td>88.82190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NM</td>\n",
       "      <td>35</td>\n",
       "      <td>New Mexico</td>\n",
       "      <td>2096829.0</td>\n",
       "      <td>20.96829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NY</td>\n",
       "      <td>36</td>\n",
       "      <td>New York</td>\n",
       "      <td>19453561.0</td>\n",
       "      <td>194.53561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NC</td>\n",
       "      <td>37</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>10488084.0</td>\n",
       "      <td>104.88084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ND</td>\n",
       "      <td>38</td>\n",
       "      <td>North Dakota</td>\n",
       "      <td>762062.0</td>\n",
       "      <td>7.62062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>OH</td>\n",
       "      <td>39</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>11689100.0</td>\n",
       "      <td>116.89100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>OK</td>\n",
       "      <td>40</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>3956971.0</td>\n",
       "      <td>39.56971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>OR</td>\n",
       "      <td>41</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>4217737.0</td>\n",
       "      <td>42.17737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>PA</td>\n",
       "      <td>42</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>12801989.0</td>\n",
       "      <td>128.01989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RI</td>\n",
       "      <td>44</td>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>1059361.0</td>\n",
       "      <td>10.59361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SC</td>\n",
       "      <td>45</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>5148714.0</td>\n",
       "      <td>51.48714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SD</td>\n",
       "      <td>46</td>\n",
       "      <td>South Dakota</td>\n",
       "      <td>884659.0</td>\n",
       "      <td>8.84659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TN</td>\n",
       "      <td>47</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>6829174.0</td>\n",
       "      <td>68.29174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TX</td>\n",
       "      <td>48</td>\n",
       "      <td>Texas</td>\n",
       "      <td>28995881.0</td>\n",
       "      <td>289.95881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>UT</td>\n",
       "      <td>49</td>\n",
       "      <td>Utah</td>\n",
       "      <td>3205958.0</td>\n",
       "      <td>32.05958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>VT</td>\n",
       "      <td>50</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>623989.0</td>\n",
       "      <td>6.23989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>VA</td>\n",
       "      <td>51</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>8535519.0</td>\n",
       "      <td>85.35519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>WA</td>\n",
       "      <td>53</td>\n",
       "      <td>Washington</td>\n",
       "      <td>7614893.0</td>\n",
       "      <td>76.14893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>WV</td>\n",
       "      <td>54</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>1792147.0</td>\n",
       "      <td>17.92147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>WI</td>\n",
       "      <td>55</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>5822434.0</td>\n",
       "      <td>58.22434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>WY</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>578759.0</td>\n",
       "      <td>5.78759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbreviation location   location_name  population   pop_100k\n",
       "1            AL       01         Alabama   4903185.0   49.03185\n",
       "2            AK       02          Alaska    731545.0    7.31545\n",
       "3            AZ       04         Arizona   7278717.0   72.78717\n",
       "4            AR       05        Arkansas   3017804.0   30.17804\n",
       "5            CA       06      California  39512223.0  395.12223\n",
       "6            CO       08        Colorado   5758736.0   57.58736\n",
       "7            CT       09     Connecticut   3565287.0   35.65287\n",
       "8            DE       10        Delaware    973764.0    9.73764\n",
       "10           FL       12         Florida  21477737.0  214.77737\n",
       "11           GA       13         Georgia  10617423.0  106.17423\n",
       "12           HI       15          Hawaii   1415872.0   14.15872\n",
       "13           ID       16           Idaho   1787065.0   17.87065\n",
       "14           IL       17        Illinois  12671821.0  126.71821\n",
       "15           IN       18         Indiana   6732219.0   67.32219\n",
       "16           IA       19            Iowa   3155070.0   31.55070\n",
       "17           KS       20          Kansas   2913314.0   29.13314\n",
       "18           KY       21        Kentucky   4467673.0   44.67673\n",
       "19           LA       22       Louisiana   4648794.0   46.48794\n",
       "20           ME       23           Maine   1344212.0   13.44212\n",
       "21           MD       24        Maryland   6045680.0   60.45680\n",
       "22           MA       25   Massachusetts   6892503.0   68.92503\n",
       "23           MI       26        Michigan   9986857.0   99.86857\n",
       "24           MN       27       Minnesota   5639632.0   56.39632\n",
       "25           MS       28     Mississippi   2976149.0   29.76149\n",
       "26           MO       29        Missouri   6626371.0   66.26371\n",
       "27           MT       30         Montana   1068778.0   10.68778\n",
       "28           NE       31        Nebraska   1934408.0   19.34408\n",
       "29           NV       32          Nevada   3080156.0   30.80156\n",
       "30           NH       33   New Hampshire   1359711.0   13.59711\n",
       "31           NJ       34      New Jersey   8882190.0   88.82190\n",
       "32           NM       35      New Mexico   2096829.0   20.96829\n",
       "33           NY       36        New York  19453561.0  194.53561\n",
       "34           NC       37  North Carolina  10488084.0  104.88084\n",
       "35           ND       38    North Dakota    762062.0    7.62062\n",
       "36           OH       39            Ohio  11689100.0  116.89100\n",
       "37           OK       40        Oklahoma   3956971.0   39.56971\n",
       "38           OR       41          Oregon   4217737.0   42.17737\n",
       "39           PA       42    Pennsylvania  12801989.0  128.01989\n",
       "40           RI       44    Rhode Island   1059361.0   10.59361\n",
       "41           SC       45  South Carolina   5148714.0   51.48714\n",
       "42           SD       46    South Dakota    884659.0    8.84659\n",
       "43           TN       47       Tennessee   6829174.0   68.29174\n",
       "44           TX       48           Texas  28995881.0  289.95881\n",
       "45           UT       49            Utah   3205958.0   32.05958\n",
       "46           VT       50         Vermont    623989.0    6.23989\n",
       "47           VA       51        Virginia   8535519.0   85.35519\n",
       "48           WA       53      Washington   7614893.0   76.14893\n",
       "49           WV       54   West Virginia   1792147.0   17.92147\n",
       "50           WI       55       Wisconsin   5822434.0   58.22434\n",
       "51           WY       56         Wyoming    578759.0    5.78759"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "theta = np.linspace(10, 60, num = 50)\n",
    "supply_limit = 0.9 * np.sum(theta * state_pops.pop_100k)\n",
    "\n",
    "def calc_theta_hat_mle(y):\n",
    "    return y\n",
    "\n",
    "def calc_theta_hat_elb(y, scale = 1.0):\n",
    "    m_hat = np.mean(y)\n",
    "    a_hat_plus_d_hat = np.var(y)\n",
    "    a_hat = a_hat_plus_d_hat - scale**2\n",
    "    c_hat = a_hat / a_hat_plus_d_hat\n",
    "    return m_hat + c_hat * (y - m_hat)\n",
    "\n",
    "nsim = 10000\n",
    "results = []\n",
    "\n",
    "for i in range(nsim):\n",
    "    z_train = np.random.normal(loc = theta, scale = 1.0)\n",
    "    y_train = z_train * state_pops.pop_100k\n",
    "    z_test = np.random.normal(loc = theta, scale = 1.0)\n",
    "    y_test = z_test * state_pops.pop_100k\n",
    "\n",
    "    theta_hat_mle = calc_theta_hat_mle(z_train)\n",
    "    theta_hat_elb = calc_theta_hat_elb(z_train)\n",
    "\n",
    "    y_hat_mle_via_pop100k = theta_hat_mle * state_pops.pop_100k\n",
    "    y_hat_elb_via_pop100k = theta_hat_elb * state_pops.pop_100k\n",
    "\n",
    "    ls_mle_via_pop100k = np.mean(norm.logpdf(y_test, loc = y_hat_mle_via_pop100k, scale=state_pops.pop_100k, ))\n",
    "    ls_elb_via_pop100k = np.mean(norm.logpdf(y_test, loc = y_hat_elb_via_pop100k, scale=state_pops.pop_100k))\n",
    "\n",
    "    crps_mle_via_pop100k = np.mean(ps.crps_gaussian(y_test, mu = y_hat_mle_via_pop100k, sig=state_pops.pop_100k))\n",
    "    crps_elb_via_pop100k = np.mean(ps.crps_gaussian(y_test, mu = y_hat_elb_via_pop100k, sig=state_pops.pop_100k))\n",
    "\n",
    "    a_hat_mle_via_pop100k = supply_limit * (y_hat_mle_via_pop100k / np.sum(y_hat_mle_via_pop100k))\n",
    "    error_mle_via_pop100k = y_test - a_hat_mle_via_pop100k\n",
    "    error_mle_via_pop100k[error_mle_via_pop100k < 0] = 0\n",
    "    as_mle_via_pop100k = np.sum(error_mle_via_pop100k)\n",
    "\n",
    "    a_hat_elb_via_pop100k = supply_limit * (y_hat_elb_via_pop100k / np.sum(y_hat_elb_via_pop100k))\n",
    "    error_elb_via_pop100k = y_test - a_hat_elb_via_pop100k\n",
    "    error_elb_via_pop100k[error_elb_via_pop100k < 0] = 0\n",
    "    as_elb_via_pop100k = np.sum(error_elb_via_pop100k)\n",
    "\n",
    "    results.append(pd.DataFrame({\n",
    "        'method': ['mle_via_pop100k', 'elb_via_pop100k'],\n",
    "        'ls': [ls_mle_via_pop100k, ls_elb_via_pop100k],\n",
    "        'crps': [crps_mle_via_pop100k, crps_elb_via_pop100k],\n",
    "        'allocation_score': [as_mle_via_pop100k, as_elb_via_pop100k]\n",
    "    }))\n",
    "\n",
    "results = pd.concat(results)\n",
    "results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>ls</th>\n",
       "      <th>crps</th>\n",
       "      <th>allocation_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mle_via_pop100k</td>\n",
       "      <td>-5.952639</td>\n",
       "      <td>63.250287</td>\n",
       "      <td>11354.650663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elb_via_pop100k</td>\n",
       "      <td>-5.937826</td>\n",
       "      <td>62.270643</td>\n",
       "      <td>11357.819677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mle_via_pop100k</td>\n",
       "      <td>-5.586159</td>\n",
       "      <td>50.871741</td>\n",
       "      <td>10829.167770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elb_via_pop100k</td>\n",
       "      <td>-5.600105</td>\n",
       "      <td>50.966789</td>\n",
       "      <td>10831.474003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mle_via_pop100k</td>\n",
       "      <td>-5.423005</td>\n",
       "      <td>55.283052</td>\n",
       "      <td>11636.300381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elb_via_pop100k</td>\n",
       "      <td>-5.614442</td>\n",
       "      <td>63.785246</td>\n",
       "      <td>10471.513871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mle_via_pop100k</td>\n",
       "      <td>-5.700609</td>\n",
       "      <td>62.940052</td>\n",
       "      <td>11898.223731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elb_via_pop100k</td>\n",
       "      <td>-5.702380</td>\n",
       "      <td>62.056551</td>\n",
       "      <td>11910.468600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mle_via_pop100k</td>\n",
       "      <td>-5.873091</td>\n",
       "      <td>66.174011</td>\n",
       "      <td>10595.338010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elb_via_pop100k</td>\n",
       "      <td>-5.881663</td>\n",
       "      <td>66.907014</td>\n",
       "      <td>10620.192655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             method        ls       crps  allocation_score\n",
       "0   mle_via_pop100k -5.952639  63.250287      11354.650663\n",
       "1   elb_via_pop100k -5.937826  62.270643      11357.819677\n",
       "0   mle_via_pop100k -5.586159  50.871741      10829.167770\n",
       "1   elb_via_pop100k -5.600105  50.966789      10831.474003\n",
       "0   mle_via_pop100k -5.423005  55.283052      11636.300381\n",
       "..              ...       ...        ...               ...\n",
       "1   elb_via_pop100k -5.614442  63.785246      10471.513871\n",
       "0   mle_via_pop100k -5.700609  62.940052      11898.223731\n",
       "1   elb_via_pop100k -5.702380  62.056551      11910.468600\n",
       "0   mle_via_pop100k -5.873091  66.174011      10595.338010\n",
       "1   elb_via_pop100k -5.881663  66.907014      10620.192655\n",
       "\n",
       "[20000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "results.groupby('method').mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ls</th>\n",
       "      <th>crps</th>\n",
       "      <th>allocation_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>elb_via_pop100k</th>\n",
       "      <td>-5.612103</td>\n",
       "      <td>53.640695</td>\n",
       "      <td>11102.059930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mle_via_pop100k</th>\n",
       "      <td>-5.614120</td>\n",
       "      <td>53.701211</td>\n",
       "      <td>11091.008124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ls       crps  allocation_score\n",
       "method                                                \n",
       "elb_via_pop100k -5.612103  53.640695      11102.059930\n",
       "mle_via_pop100k -5.614120  53.701211      11091.008124"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "According to these results, the MLE method scores worse according to log score (it has a lower log score) and CRPS (it has a higher CRPS), but better according to the application-specific allocation score (it has a lower allocation score). Our intuition is that this is because the shrinkage method helps both log score and CRPS, as indicated in the theory above, but it is harmful for the allocation score. Shrinkage is harmful to the allocation score because it tends to shrink large estimates down, resulting in systematic under-allocation of resources to the locations with largest incidence.\n",
    "\n",
    "Follow up questions:\n",
    " * What about other estimators?\n",
    " * What about other data generating processes?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Older commentary below.\n",
    "\n",
    "### Reason for concern and possible next steps\n",
    "\n",
    "The above suggests that if log scores are used for measuring forecast skill, shrinkage estimators may be rewarded. The details need to be worked out, but there are a few potential concerns here:\n",
    "\n",
    "1. If we're measuring forecast skill across many locations, this may reward systematic underprediction of incidence in the locations with largest incidence. This would not be desirable from the perspective of an epidemiologist working in those locations.\n",
    "2. If we're measuring forecast skill over time, this may reward systematic underprediction of incidence at times with largest incidence (indeed, this is exactly what a Kalman filter will tend to do). This would not be desirable from the perspective of a public health decision maker; e.g., it may be important to obtain unbiased estimates of peak hospitalizations, and this may not be what arises from forecasts of peak hospitalizations that are obtained from a model that is trained to optimize a rule that encourages shrinkage at the peaks.\n",
    "3. I think the CRPS in particular may interact strangely with processes that occur with exponential growth. As used in the forecasting exercises, the CRPS measures forecast skill on the scale of observed cases or deaths. If a model estimates the growth rate of a process with exponential growth, errors on the high side will likely result in larger penalties than errors on the low side. i.e. if the growth rate is 3, an estimate of 3.1 may result in lower scores than an estimate of 2.9. This may induce a systematic downward bias in growth rate estimates. Can we compare to a score directly measuring estimates of growth rate?\n",
    "\n",
    "There are also some follow up questions to the above:\n",
    "\n",
    "1. What happens to the above if $\\sigma^2$ is not known?\n",
    "1. What happens to the above if the observations are not independent?  Suppose they arise from a linear Gaussian time series process (i.e., the set up for a Kalman filter) to start.\n",
    "1. What happens if there is more structure in the covariance for the random vector observed at each time?\n",
    "1. What happens if there are different variances for each component?  For example, suppose you have $p$ locations with different populations, and the mean and variance both scale with the population size?  We think it should be optimal from an MSE/log score perspective to do more shrinkage for larger locations.\n",
    "1. What happens to the above if observations don't follow a normal distribution? Something skewed like a log-normal?  Something discrete like a Poisson or negative binomial?\n",
    "1. Bayesian formulation of all of the above.\n",
    "1. Other scoring rules (e.g. CRPS/WIS)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}